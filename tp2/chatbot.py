"""
Chatbot con Memoria Persistente usando LangChain, Pinecone y Groq
=====================================================

Este archivo implementa un chatbot con interfaz web usando Streamlit que:
- Mantiene memoria de conversaciones anteriores
- Utiliza diferentes modelos de LLM a trav√©s de Groq
- Permite personalizaci√≥n del comportamiento del bot
- Gestiona la memoria conversacional autom√°ticamente

Tecnolog√≠as utilizadas:
- Streamlit: Para la interfaz web
- LangChain: Para gesti√≥n de memoria y cadenas de conversaci√≥n
- Groq: Como proveedor de modelos LLM
- Pinecone: Base de datos vectorial para contexto relevante
- Python: Lenguaje de programaci√≥n

Instrucciones para ejecutar:
    streamlit run chatbot.py

Variables de entorno necesarias:
    GROQ_API_KEY: Tu clave API de Groq (obtener en https://console.groq.com)
    PINECONE_API_KEY: Tu clave API de Pinecone (obtener en https://www.pinecone.io/)
"""

# ========================================
# IMPORTACI√ìN DE LIBRER√çAS NECESARIAS
# ========================================

import streamlit as st           # Framework para crear aplicaciones web interactivas
import os                      # Para acceso a variables de entorno

# Importaciones espec√≠ficas de LangChain para gesti√≥n de conversaciones

from langchain_core.prompts import (
    ChatPromptTemplate,           # Template para estructurar mensajes de chat
    MessagesPlaceholder,          # Marcador de posici√≥n para el historial
)
from langchain_groq import ChatGroq              # Integraci√≥n LangChain-Groq
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_huggingface import HuggingFaceEmbeddings  # Modelo de embeddings
from langchain_pinecone import PineconeVectorStore # Integraci√≥n LangChain-Pinecone
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

def main():
    """
    Funci√≥n principal de la aplicaci√≥n de chatbot.
    
    Esta funci√≥n coordina todos los componentes del chatbot:
    1. Configuraci√≥n de la interfaz de usuario
    2. Gesti√≥n de la memoria conversacional
    3. Integraci√≥n con el modelo de lenguaje
    4. Procesamiento de preguntas y respuestas
    
    Funcionalidades principales:
    - Interfaz web responsiva con Streamlit
    - Memoria de conversaci√≥n con longitud configurable
    - Selecci√≥n de diferentes modelos LLM
    - Personalizaci√≥n del prompt del sistema
    - Historial persistente durante la sesi√≥n
    """
    
    # ========================================
    # CONFIGURACI√ìN INICIAL Y AUTENTICACI√ìN
    # ========================================
    
    # Obtener la clave API de Groq desde las variables de entorno
    # Esto es una pr√°ctica de seguridad recomendada para no exponer credenciales en el c√≥digo
    groq_api_key = os.getenv('GROQ_API_KEY')
    
    # Verificar si la clave API est√° configurada
    if not groq_api_key:
        st.error("‚ö†Ô∏è GROQ_API_KEY no est√° configurada en las variables de entorno")
        st.info("üí° Configura tu clave API: export GROQ_API_KEY='tu-clave-aqui'")
        st.stop()  # Detener la ejecuci√≥n si no hay clave API

    pinecone_api_key = os.getenv('PINECONE_API_KEY')

    if not pinecone_api_key:
        st.error("‚ö†Ô∏è PINECONE_API_KEY no est√° configurada en las variables de entorno")
        st.info("üí° Configura tu clave API: export PINECONE_API_KEY='tu-clave-aqui'")
        st.stop()  # Detener la ejecuci√≥n si no hay clave API

    # ========================================
    # CONFIGURACI√ìN DE LA INTERFAZ PRINCIPAL
    # ========================================
    
    # Configurar el t√≠tulo y descripci√≥n de la aplicaci√≥n
    st.title("ü§ñ Chatbot del CV de Kevin Cajachu√°n")
    st.markdown("""
    **¬°Bienvenido al chatbot!** 
    
    Este chatbot utiliza:
    - üß† **Memoria conversacional**: Recuerda el contexto de tu conversaci√≥n
    - üîÑ **Modelos intercambiables**: Puedes elegir diferentes LLMs
    - ‚öôÔ∏è **Personalizaci√≥n**: Configura el comportamiento del asistente
    - üöÄ **Powered by Groq**: Respuestas r√°pidas y precisas
    - üìö **Base de datos vectorial**: Contexto relevante para mejores respuestas
    """)

    # ========================================
    # PANEL DE CONFIGURACI√ìN LATERAL
    # ========================================
    
    st.sidebar.title('‚öôÔ∏è Configuraci√≥n del Chatbot')
    st.sidebar.markdown("---")
    
    model = 'llama-3.1-8b-instant'
    
    # Control deslizante para la longitud de memoria
    st.sidebar.subheader("üß† Configuraci√≥n de Memoria")
    conversational_memory_length = st.sidebar.slider(
        'Longitud de la memoria conversacional:', 
        min_value=1, 
        max_value=10, 
        value=5,
        help="N√∫mero de intercambios anteriores que el bot recordar√°. M√°s memoria = mayor contexto pero mayor costo computacional"
    )
    
    # Mostrar informaci√≥n sobre la memoria
    st.sidebar.caption(f"üí≠ El bot recordar√° los √∫ltimos {conversational_memory_length} intercambios")

    # ========================================
    # CONFIGURACI√ìN DE LA MEMORIA CONVERSACIONAL
    # ========================================
    
    # Nueva API: gestionamos historial con RunnableWithMessageHistory + InMemoryChatMessageHistory
    if "session_id" not in st.session_state:
        st.session_state.session_id = "default"
    if "history_store" not in st.session_state:
        st.session_state.history_store = {}
    
    # ========================================
    # GESTI√ìN DEL HISTORIAL DE CONVERSACI√ìN
    # ========================================
    
    # Inicializar el historial de chat en el estado de la sesi√≥n de Streamlit
    # st.session_state permite mantener datos entre ejecuciones de la aplicaci√≥n
    if 'historial_chat' not in st.session_state:
        st.session_state.historial_chat = []
        st.sidebar.success("üí¨ Nueva conversaci√≥n iniciada")
    else:
        # Mostrar informaci√≥n del historial en la barra lateral
        st.sidebar.info(f"üí¨ Conversaci√≥n con {len(st.session_state.historial_chat)} mensajes")
    
    # Bot√≥n para limpiar el historial
    if st.sidebar.button("üóëÔ∏è Limpiar Conversaci√≥n"):
        st.session_state.historial_chat = []
        # Reiniciar historial de LangChain para la sesi√≥n actual
        sid = st.session_state.session_id
        if sid in st.session_state.history_store:
            st.session_state.history_store[sid] = InMemoryChatMessageHistory()
        st.sidebar.success("‚úÖ Conversaci√≥n limpiada")
        st.rerun()  # Recargar la aplicaci√≥n
    
    # ========================================
    # INTERFAZ DE ENTRADA DEL USUARIO
    # ========================================
    
    # Crear el campo de entrada para las preguntas del usuario
    user_question = st.chat_input("Escribe tu mensaje aqu√≠...")

    # ========================================
    # CONFIGURACI√ìN DEL MODELO DE LENGUAJE
    # ========================================
    
    # Inicializar el cliente de ChatGroq con las configuraciones seleccionadas
    try:
        groq_chat = ChatGroq(
            groq_api_key=groq_api_key,     # Clave API para autenticaci√≥n
            model_name=model,              # Modelo seleccionado por el usuario
            temperature=0.7,               # Creatividad de las respuestas (0=determinista, 1=creativo)
            max_tokens=1000,               # M√°ximo n√∫mero de tokens en la respuesta
        )
        st.sidebar.success("‚úÖ Modelo conectado correctamente")
    except Exception as e:
        st.sidebar.error(f"‚ùå Error al conectar con Groq: {str(e)}")
        st.stop()

    # ========================================
    # CONFIGURACI√ìN DEL EMBEDDING
    # ========================================

    embed_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # ========================================
    # CONFIGURACI√ìN DE LA DB VECTORIAL
    # ========================================

    index_name = 'kajachuan'
    namespace = "nlp2"

    vectorstore = PineconeVectorStore(
        pinecone_api_key = pinecone_api_key,
        index_name=index_name,
        embedding=embed_model,
        namespace=namespace,
    )
    retriever=vectorstore.as_retriever()

    # ========================================
    # PROCESAMIENTO DE LA PREGUNTA Y RESPUESTA
    # ========================================

    # Si el usuario ha hecho una pregunta,
    if user_question:

        # Mostrar indicador de carga mientras se procesa
        with st.spinner('ü§î El chatbot est√° pensando...'):
            
            try:
                system_prompt = (
                    "Eres un asistente para tareas de preguntas y respuestas. "
                    "Usa las siguientes partes del contexto recuperado para responder"
                    "la pregunta. Si no sabes la respuesta, di que no lo sabes."
                    "Usa un m√°ximo de tres oraciones y mant√©n la respuesta concisa."
                    "\n\n"
                    "{context}"
                )
                # ========================================
                # CONSTRUCCI√ìN DEL TEMPLATE DE CONVERSACI√ìN
                # ========================================
                
                # Crear un template de chat que incluye:
                # 1. Mensaje del sistema (personalidad/instrucciones)
                # 2. Historial de conversaci√≥n (memoria)
                # 3. Mensaje actual del usuario
                prompt = ChatPromptTemplate.from_messages([
                    
                    # Mensaje del sistema - Define el comportamiento del chatbot
                    ("system", system_prompt),
                    
                    # Marcador de posici√≥n para el historial - Se reemplaza autom√°ticamente
                    MessagesPlaceholder(variable_name="historial_chat"),
                    
                    # Template para el mensaje actual del usuario
                    ("human", "{input}")
                ])
                
                # ========================================
                # CREACI√ìN DE LA CADENA DE CONVERSACI√ìN
                # ========================================
                
                question_answer_chain = create_stuff_documents_chain(groq_chat, prompt)
                rag_chain = create_retrieval_chain(retriever, question_answer_chain)
                
                # Obtener/crear historial de la sesi√≥n actual
                session_id = st.session_state.session_id
                store = st.session_state.history_store
                if session_id not in store:
                    store[session_id] = InMemoryChatMessageHistory()
                
                # Envolver con memoria conversacional usando la nueva API
                chain_with_memory = RunnableWithMessageHistory(
                    rag_chain,
                    lambda sid: store.setdefault(sid, InMemoryChatMessageHistory()),
                    input_messages_key="input",
                    history_messages_key="historial_chat",
                    output_messages_key="answer",
                )
                
                # ========================================
                # GENERACI√ìN DE LA RESPUESTA
                # ========================================
                
                # Enviar la pregunta al modelo y obtener la respuesta
                result = chain_with_memory.invoke(
                    {"input": user_question},
                    config={"configurable": {"session_id": session_id}},
                )
                response = result["answer"]
                
                # ========================================
                # ALMACENAMIENTO Y VISUALIZACI√ìN
                # ========================================
                
                st.session_state.historial_chat.append({"role": "user", "content": user_question})
                st.session_state.historial_chat.append({"role": "bot", "content": response})
                
                # ========================================
                # MOSTRAR LA CONVERSACI√ìN
                # ========================================
                
                # Mostrar la respuesta actual destacada
                st.markdown("### üí¨ Conversaci√≥n:")

                for msg in st.session_state.historial_chat:
                    role = msg["role"]
                    content = msg["content"]

                    if role == "user":
                        st.chat_message("user").markdown(content)
                    else:
                        st.chat_message("assistant").markdown(content)
                
                # Informaci√≥n adicional sobre la respuesta
                st.caption(f"üìä Modelo: {model} | üß† Memoria: {conversational_memory_length} mensajes")
                
            except Exception as e:
                # Manejo de errores durante el procesamiento
                st.error(f"‚ùå Error al procesar la pregunta: {str(e)}")
                st.info("üí° Verifica tu conexi√≥n a internet y la configuraci√≥n de la API")


if __name__ == "__main__":
    # Punto de entrada de la aplicaci√≥n
    # Solo ejecutar main() si este archivo se ejecuta directamente
    main()
